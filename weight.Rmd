---
title: "Weight Lifting Exercise" 
output: html_document
---

In this project we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6
participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways
(see (http://groupware.les.inf.puc-rio.br/har) under Weight Lifting Exercise Dataset):

- Class A: exactly according to the specification
- Class B: throwing the elbows to the front 
- Class C: lifting the dumbbell only halfway
- Class D: lowering the dumbbell only halfway
- Class E: throwing the hips to the front

This project is aimed to train a learning machine to correctly predict and classify these tasks
when given the activation pattern of the accelerometers.


## Loading and preprocessing the data

```{r, warning=FALSE, message=FALSE}
library(caret); library(dplyr);
alldata <- read.csv("pml-training.csv")
```

The following command reveals that there are 100 variables which are rarely assigned:

```{r}
notna <- t(summarise_each(data.frame(!(is.na(alldata) | alldata =="") ), funs(sum)))
table(notna)
```

We will exclude these variables from learning. 
Furthermore, the learning should not be made on the names of the participants or
the timestamps of the lifting excercise. So we exclude them:

```{r}
nouseVar <- c(names(alldata)[1:7],rownames(notna)[notna<=500])
usedVar <- which(!names(alldata) %in% nouseVar)
gooddata <- select(alldata, usedVar)
```

So the variable `gooddata` contains all the activation pattern from the accelerometers.
The classifier (i.e. labels `A` to `E`) is in the 53rd column: `gooddata[,53]` or just `gooddata$classe`.

## Splitting in training and test data set

For testing the data, be sure about the in-sample-error and 
out-of-sample-error and for demonstration, we will do both cross-validation with 
`10` groups (see later) and a test on a separate data set composed of 40% of the data. 
So 60% of the given data will be used for training.

```{r, warning=FALSE, message=FALSE}
set.seed(12345)
inTrain <- createDataPartition(gooddata$classe, p=0.6, list=FALSE)
traindata <- gooddata[inTrain,]
testdata <- gooddata[-inTrain,]
```

## Exploring the training data and choosing the learning method

The following extract of the data indicates partially high correlation under the 
variables. Furthermore, we see highly non-linear structures in the data and points
from different classes clump together.

```{r, warning=FALSE, message=FALSE}
library(GGally)
plotdata <- traindata[createDataPartition(traindata[,1], p=0.01, list=FALSE),]
ggpairs(plotdata, columns=c(1:6,53), color = "classe", title="Feature Plot", alpha=0.4, columnLabels=rep("",7))
```

So the reducing of the features seem to take long and be difficult and 
(general) linear models seem to be unappropriate for this classification task.
So we need a method which is robust against correlations in the features
and can deal with non-linear structures and still is sensitive to little changes.

For that reason the classification with the random forest method seems to be
promissing (see [http://www.stat.berkeley.edu/~breiman/RandomForests/]).


## Training and testing 

We will use the random forest method to train the machine.
The accuracy is supposed to get better with a higher number of (uncorrelated) trees.
Since the computer resources are sparse we will limit ourselves to 20 trees.
The algorithm automatically then calculates the optimal number `mtry` used for
variable selection at each node of the trees.

It is said in the literature that cross validation for random forests 
actually is not needed to get a good out-of-sample-error approximation.
The reason is that the data is sampled with replacement and that usually one third
of the date is not used for building the classification trees. The remaining data
(called "out out bag" data) is then internally used for testing the machine and
calculating the the so called "out of bag" error (OOB). 
And the "out-of-bag" (OOB) error is said to be a good approximation for the out-of-sample-error.

Nevertheless we will use cross validation for demonstration.
Since there is too much given data an exhaustive cross validation (e.g. "leave-one-out")
is not feasible.
So we restrict ourselves to 10-fold cross validation, i.e. the training data is
sliced randomly into 10 (equally big) groups. Then the machine is run 10 times. Everytime
it is trained on 9 of these groups and tested against the remaining one.
This then gives an approximation for the out-of-sample-error.

And actually a previous special split into test and training data is not necessary.
But we will also do this for demonstration.

### Training without cross validation

First we build the random forest without cross validation:

```{r, warning=FALSE, message=FALSE}
modelFit1 <- train(y=traindata[,53], x=traindata[,-53], method="rf", ntree=20)
print(modelFit1$finalModel)
```

### Training with 10-fold cross validation

Then we compare it to the random forest with 10-fold cross validation:

```{r, warning=FALSE, message=FALSE}
control <- trainControl(method="cv", 10)
modelFit2 <- train(y=traindata[,53], x=traindata[,-53], method="rf", trControl=control, ntree=20)
print(modelFit2$finalModel)
```

### Comparison with the test set

The confusion matrix on the test data looks like the following on our models:

```{r, warning=FALSE, message=FALSE}
prediction1 <- predict(modelFit1, testdata[,-53])
prediction2 <- predict(modelFit2, testdata[,-53])
cbind(confusionMatrix(testdata[,53], prediction1)$table,
      confusionMatrix(testdata[,53], prediction2)$table)
```

The accuracies are as follows:

```{r, warning=FALSE, message=FALSE}
cbind(confusionMatrix(testdata[,53], prediction1)$overall[1], confusionMatrix(testdata[,53], prediction2)$overall[1])
```

We see that in all cases (with or without crossvalidation) the accuracy of the 
learning machine (on test and training set and cross validated sets) is above 98% and
the out-of-sample-error can be estimated to be below 2% (in all cases).
If we did not do the actual test we would assume heavy overfitting with
these values (98% is very high).
But since we used 3 methods and a big test set the learning machine seems to be very accurate.

## Prediction about the validation data

We also give the prediction of the validation data set for both models.

```{r}
validationdata <- read.csv("pml-testing.csv") 
valdata <- select(validationdata, usedVar)
predict(modelFit1, valdata[,-53])
predict(modelFit2, valdata[,-53])
```

These are the labels of the 20 validation cases in the given row.

To write the answers into files the following code was copied from coursera's course page:

```{r, eval =FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
```

Write the answers into  files:

```{r, eval=FALSE}
pml_write_files(predict(modelFit1, valdata[,-53]))
```

## References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 


